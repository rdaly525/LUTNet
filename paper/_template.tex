% $Id: template.tex 11 2007-04-03 22:25:53Z jpeltier $

\documentclass{vgtc}                          % final (conference style)
%\documentclass[review]{vgtc}                 % review
%\documentclass[widereview]{vgtc}             % wide-spaced review
%\documentclass[preprint]{vgtc}               % preprint
%\documentclass[electronic]{vgtc}             % electronic version

%% Uncomment one of the lines above depending on where your paper is
%% in the conference process. ``review'' and ``widereview'' are for review
%% submission, ``preprint'' is for pre-publication, and the final version
%% doesn't use a specific qualifier. Further, ``electronic'' includes
%% hyperreferences for more convenient online viewing.

%% Please use one of the ``review'' options in combination with the
%% assigned online id (see below) ONLY if your paper uses a double blind
%% review process. Some conferences, like IEEE Vis and InfoVis, have NOT
%% in the past.

%% Figures should be in CMYK or Grey scale format, otherwise, colour 
%% shifting may occur during the printing process.

%% These few lines make a distinction between latex and pdflatex calls and they
%% bring in essential packages for graphics and font handling.
%% Note that due to the \DeclareGraphicsExtensions{} call it is no longer necessary
%% to provide the the path and extension of a graphics file:
%% \includegraphics{diamondrule} is completely sufficient.
%%
\ifpdf%                                % if we use pdflatex
  \pdfoutput=1\relax                   % create PDFs from pdfLaTeX
  \pdfcompresslevel=9                  % PDF Compression
  \pdfoptionpdfminorversion=7          % create PDF 1.7
  \ExecuteOptions{pdftex}
  \usepackage{graphicx}                % allow us to embed graphics files
  \DeclareGraphicsExtensions{.pdf,.png,.jpg,.jpeg} % for pdflatex we expect .pdf, .png, or .jpg files
\else%                                 % else we use pure latex
  \ExecuteOptions{dvips}
  \usepackage{graphicx}                % allow us to embed graphics files
  \DeclareGraphicsExtensions{.eps}     % for pure latex we expect eps files
\fi%

%% it is recomended to use ``\autoref{sec:bla}'' instead of ``Fig.~\ref{sec:bla}''
\graphicspath{{figures/}{pictures/}{images/}{./}} % where to search for the images

\usepackage{microtype}                 % use micro-typography (slightly more compact, better to read)
\PassOptionsToPackage{warn}{textcomp}  % to address font issues with \textrightarrow
\usepackage{textcomp}                  % use better special symbols
\usepackage{mathptmx}                  % use matching math font
\usepackage{times}                     % we use Times as the main font
\renewcommand*\ttdefault{txtt}         % a nicer typewriter font
\usepackage{cite}                      % needed to automatically sort the references
\usepackage{tabu}                      % only used for the table example
\usepackage{booktabs}                  % only used for the table example
\usepackage{xcolor}
%% We encourage the use of mathptmx for consistent usage of times font
%% throughout the proceedings. However, if you encounter conflicts
%% with other math-related packages, you may want to disable it.


%% If you are submitting a paper to a conference for review with a double
%% blind reviewing process, please replace the value ``0'' below with your
%% OnlineID. Otherwise, you may safely leave it at ``0''.
\onlineid{0}

%% declare the category of your paper, only shown in review mode
\vgtccategory{Research}

%% allow for this line if you want the electronic option to work properly
\vgtcinsertpkg

%% In preprint mode you may define your own headline.
%\preprinttext{To appear in an IEEE VGTC sponsored conference.}

%% Paper title.

\title{Hardware-Software Co-Design for DNNs on FPGAs}

%% This is how authors are specified in the conference style

%% Author and Affiliation (single author).
%%\author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com}}
%%\affiliation{\scriptsize Allied Widgets Research}

%% Author and Affiliation (multiple authors with single affiliations).
\author{Ross Daly\thanks{e-mail: ross.daly@stanford.edu} %
\affiliation{\scriptsize Stanford University}

%% Author and Affiliation (multiple authors with multiple affiliations)
%% \author{Roy G. Biv\thanks{e-mail: roy.g.biv@aol.com}\\ %
%%         \scriptsize Starbucks Research %
%% \and Ed Grimley\thanks{e-mail: ed.grimley@aol.com}\\ %
%%      \scriptsize Grimley Widgets, Inc. %
%% \and Martha Stewart\thanks{e-mail: martha.stewart@marthastewart.com}\\ %
%%      \parbox{1.4in}{\scriptsize \centering Martha Stewart Enterprises \\ Microsoft Research}}

%% A teaser figure can be included as follows, but is not recommended since
%% the space is now taken up by a full width abstract.
%\teaser{
%  \includegraphics[width=1.5in]{sample.eps}
%  \caption{Lookit! Lookit!}
%}

\usepackage{todonotes}
%% Override TODO MACRO to be simpler inline version
%% \newcommand{\todo}[1]{}
\renewcommand{\todo}[1]{{\color{red} TODO: {#1}}}

\usepackage{subcaption}

%% Abstract section.
\input{abstract}
% end of abstract

%% ACM Computing Classification System (CCS). 
%% See <http://www.acm.org/about/class> for details.
%% We recommend the 2012 system <http://www.acm.org/about/class/class/2012>
%% For the 2012 system use the ``\CCScatTwelve'' which command takes four arguments.
%% The 1998 system <http://www.acm.org/about/class/class/2012> is still possible
%% For the 1998 system use the ``\CCScat'' which command takes four arguments.
%% In both cases the last two arguments (1998) or last three (2012) can be empty.

% \CCScatlist{
%   \CCScatTwelve{Human-centered computing}{Visu\-al\-iza\-tion}{Visu\-al\-iza\-tion techniques}{Treemaps};
%   \CCScatTwelve{Human-centered computing}{Visu\-al\-iza\-tion}{Visualization design and evaluation methods}{}
% }

%\CCScatlist{
  %\CCScat{H.5.2}{User Interfaces}{User Interfaces}{Graphical user interfaces (GUI)}{};
  %\CCScat{H.5.m}{Information Interfaces and Presentation}{Miscellaneous}{}{}
%}

%% Copyright space is enabled by default as required by guidelines.
%% It is disabled by the 'review' option or via the following command:
% \nocopyrightspace

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% START OF THE PAPER %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%% The ``\maketitle'' command must be the first command after the
%% ``\begin{document}'' command. It prepares and prints the title block.

%% the only exception to this rule is the \firstsection command
\firstsection{Introduction}
Hardware-Software Co-Design for Neural Networks on FPGAs

Due to the predictable end of Moores law and Denard Scaling, existing software abstractions will need to break down in order to leverage new hardware accelerators and existing reconfigurable architectures like FPGAs. Efficiently using existing and new hardware for domains such as Deep Learning is an important research area.

Mapping DNNs to FPGAs is an open research problem. DNNs contain a variety of computation such as Convolutional, Fully Connected, NonLinearities (ReLU/Tanh), BatchNorm, layer norm, downsampling (Max Pooling/Strided convolutions), etc. These layers are typically trained and run using floating point operations. Since floating point operations are expensive in both area and power, a large set of work has been done on quantizing floating point operators into computationally simpler and lower power fixed point operations while still maintaining DNN accuracy. There has also been some work on taking this to the extreme and doing 1-bit operations (XNOR-net). In addition, since full networks typically will not fit on an FPGA, more generic tensor processing elements can be used and computations are then scheduled in time.

While this strategy of quantization broaches the problem of software-hardware codesign, it does not fully embrace it for FPGAs.
Running any of these operations requires some mapping between the floating point/fixed point operators and the fundamental computational blocks of the FPGA. The computational parts of FPGAs are the configurable Look Up Tables (LUTs) which are plentiful, and Fixed Function blocks which are limited in kind and quantity. This mapping of Tensor OPs is not always precise which means the amount of LUTs will be underutilized as well as each LUT itself being underutilized.

We can do better. Lets bake in the fact that we are utilizing LUTs for computation at the software-algorithmic level. Concretely I propose reforming the DNN optimization problem as learning the LUT configurations directly rather than learning weights and indirectly mapping down to LUTs. This approach can be used to either replace certain layers in DNNs, or as a full replacement of the DNN. 

This may sound crazy because there are numerous different types of configuration state (LUT values, mux selects, register initialization, Memories, connection network, etc) as well as it being a large discrete optimization problem.

I have developed a methodology for tackling this problem by doing the following. First constrain the problem to be a static architecture of LUTs (called a LUTNet). This reduces the learnable configuration state to only the values of the LUTs. Second: relax this discrete problem to transform it into a continuous one which allows me to leverage typical machine learning optimization techniques such as SGD. I can then use quantization techniques to transform achieve the discrete solution. In the open questions section I address some thoughts on removing the static architecture constraint.

I have implemented a first pass of this idea in Tensorflow. Preliminary results are that I can train and achieve around 94% accuracy on MNIST using a static architecture of ~1500 4-bit LUTs. This would fit on even the smallest FPGA. I have also seen evidence that there is a nice Pareto curve between accuracy and number of LUTs which could drive accuracy higher. I have not done any direct comparisons between other FPGA implementations yet, but some back of the hand calculations comparing an MNIST softmax implementation seems very positive.

Even though these LUT networks are different from DNNs, they look and feel similar. I do not have to completely throw out the DNN literature. I can construct layers that look and feel like fully connected, convolution, and down sample layers.  In fact the MNIST architecture is simply using two “fully-connected” LUT Layers.

In this project I would like to explore some or all of the following;
How well does this approach scale to non-toy datasets like ImageNet.
Taking a subset of existing state of the art architectures like MobileNet and replace that part with a LUTNet.
Expanding the LUT architecture to include FPGA fixed function blocks.
Reproducing and decreasing area of quantized networks like XNOR-net by framing them as LUTs and Fixed-functions
Exploring methods for formulating LUT connections as learnable. This could also be relevant to normal DNNs. 
Integrate state (registers and memories) into these LUT architectures.
\maketitle

Eschbach et al.~\cite{eschbach2005orthogonal}

%% \section{Introduction} %for journal use above \firstsection{..} instead

\section{Related Work}

\section{Methods}

\section{Results}

\section{Discussion}

\section{Future Work}

\bibliographystyle{abbrv-doi}

\bibliography{template}
\end{document}
